{"cells":[{"cell_type":"markdown","source":["# Bienvenido a este Notebook\n","## Vamos a explorar datos con PySpark (python) para crear un DataMart de ejemplo "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"70c23b23-8f9a-4cb4-9d3d-f3b774e15013"},{"cell_type":"code","source":["import pandas as pd \n","\n","folder_path = \"Files\"\n","\n","# Leer todos los Parquets en una carpeta\n","df = spark.read.format(\"parquet\").option(\"header\", \"true\").load(folder_path)\n","\n","# Mostrar los datos cargados\n","display(df)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"7a62d38f-06f7-427a-a256-5787f8203b14"},{"cell_type":"markdown","source":["## Ahora guardaremos los datos en las tablas del Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6d8f83f3-03b2-4999-8dfb-94068cafbfaf"},{"cell_type":"code","source":["# Especificar el nombre de la tabla en el Lakehouse\n","table_name = \"LH_Taller.Landing\"\n","\n","# Escribir los datos en el esquema de tablas del Lakehouse con opción mergeSchema\n","df.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(table_name)\n","\n","print(f\"Los datos se han guardado en la tabla '{table_name}' en el Lakehouse.\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b486a5f1-4ae2-46ac-9082-cd1ad0c8815b"},{"cell_type":"code","source":["df = spark.sql(\"SELECT * FROM LH_Taller.landing LIMIT 100\")\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"c4643567-08c6-4b12-b628-cff98f6d331d"},{"cell_type":"markdown","source":["# intentemos ahora con datos de la Web"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"20bc4387-adad-4f21-9c88-a7dad7a97837"},{"cell_type":"code","source":["import requests\n","\n","# URL de la página\n","url = \"https://si3.bcentral.cl/Indicadoressiete/secure/Serie.aspx?gcode=PRE_EUR&param=cgBnAE8AOQBlAGcAIwBiAFUALQBsAEcAYgBOAEkASQBCAEcAegBFAFkAeABkADgASAA2AG8AdgB2AFMAUgBYADIAQwBzAEEARQBMAG8ASgBWADQATABrAGQAZAB1ADIAeQBBAFAAZwBhADIAbABWAHcAXwBXAGgATAAkAFIAVAB1AEIAbAB3AFoAdQBRAFgAZwA5AHgAdgAwACQATwBZADcAMwAuAGIARwBFAFIASwAuAHQA\"\n","\n","# Hacer la solicitud HTTP\n","response = requests.get(url)\n","\n","# Verificar el estado de la respuesta\n","if response.status_code == 200:\n","    print(\"Datos obtenidos exitosamente\")\n","    # Guardar el contenido de la respuesta\n","    with open(\"datos.html\", \"w\", encoding=\"utf-8\") as file:\n","        file.write(response.text)\n","else:\n","    print(f\"Error al acceder a la URL: {response.status_code}\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"19ed2fa8-fc42-47fa-8654-3717651c83f0"},{"cell_type":"code","source":["import pandas as pd\n","from pyspark.sql import SparkSession\n","\n","# Crear sesión de Spark\n","spark = SparkSession.builder.getOrCreate()\n","\n","# Leer el archivo HTML guardado\n","dataframes = pd.read_html(\"datos.html\")  \n","\n","# Mostrar las tablas encontradas\n","for i, df in enumerate(dataframes):\n","    print(f\"Tabla {i}:\\n\", df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"e5394844-1691-48b9-9d01-c8fd9a16f209"},{"cell_type":"code","source":["# Convertir la Tabla1 (índice 1) a formato Spark DataFrame\n","table1_pandas = dataframes[1]\n","table1_spark = spark.createDataFrame(table1_pandas)\n","\n","# Definir la carpeta en el Lakehouse\n","parquet_folder = \"Files/eurbcentral\"\n","\n","# Guardar la Tabla1 como archivo Parquet en el Lakehouse\n","table1_spark.write.mode(\"overwrite\").parquet(parquet_folder)\n","\n","print(f\"Tabla1 guardada exitosamente en formato Parquet en '{parquet_folder}'.\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8bc52a5c-465b-4347-8da4-2c9ac9c7cf64"},{"cell_type":"markdown","source":["## Ahora vamos a trabajar esa tabla importada desde la web, la desdinamizaremos y la escribiremos en las tablas del lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f0b269c8-a6e9-4087-a2c8-619aab908c7d"},{"cell_type":"code","source":["ruta_eurclp = \"Files/eurbcentral\"\n","\n","df_eur = spark.read.format(\"parquet\").option(\"header\",\"true\").load(ruta_eurclp)\n","display(df_eur)\n","\n","print(\"Columnas originales:\", df_eur.columns)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"414c1883-5060-4b8a-882f-734306e35a52"},{"cell_type":"code","source":["df_eur = df_eur.toDF(*[col_name.replace(\"Ã\", \"i\").replace(\"\\xada\", \"a\").replace(\" \", \"_\") for col_name in df_eur.columns])\n","\n","# Verifica las columnas corregidas\n","print(\"Columnas corregidas:\", df_eur.columns)\n","\n","\n","# Ajusta los nombres de las columnas para que no contengan caracteres especiales\n","columns = [col_name.strip().replace(\"Dia\", \"day\").replace(\" \",\"_\") for col_name in df_eur.columns]\n","\n","df_eur = df_eur.toDF(*columns)\n","print(\"nuevos nombres:\", df_eur.columns)\n","\n","#display(df_eur)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"e8d9cbbd-7b72-4d28-897d-8520cd5cc0ac"},{"cell_type":"code","source":["# Realiza la transformación pivot inversa (melt)\n","\n","from pyspark.sql.functions import col\n","\n","# Convertir todas las columnas de los meses a STRING\n","columnas_meses = ['Enero', 'Febrero', 'Marzo', 'Abril', 'Mayo', 'Junio', 'Julio', 'Agosto', \n","                  'Septiembre', 'Octubre', 'Noviembre', 'Diciembre']\n","\n","for columna in columnas_meses:\n","    df_eur = df_eur.withColumn(columna, col(columna).cast(\"string\"))\n","\n","\n","melted_df = df_eur.selectExpr(\n","    \"day as dia\",\n","    \"stack(12, 'Enero', Enero, 'Febrero', Febrero, 'Marzo', Marzo, 'Abril', Abril, \\\n","                 'Mayo', Mayo, 'Junio', Junio, 'Julio', Julio, 'Agosto', Agosto, \\\n","                 'Septiembre', Septiembre, 'Octubre', Octubre, 'Noviembre', Noviembre, \\\n","                 'Diciembre', Diciembre) as (mes, valor)\"\n",")\n","# Limpia los datos reemplazando los valores nulos por 0\n","melted_df = melted_df.fillna({'valor': 0})\n","\n","# Muestra el resultado final\n","display(melted_df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"fd22a205-11e5-4e7f-b7b2-c93d33b882ab"},{"cell_type":"code","source":["melted_df.select(\"mes\").distinct().show()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6a451e3e-29f0-40a0-b4a2-2b84bfd23dc5"},{"cell_type":"code","source":["# vamos a limpiar estos datos y dejarlos listos para escribirlos en tabla\n","from pyspark.sql.functions import col, concat_ws, to_date, lit, when , regexp_replace, create_map, lpad\n","\n","spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n","\n","# Creamos un diccionario para mapear meses\n","month_mapping = {\n","    \"Enero\": \"01\", \"Febrero\": \"02\", \"Marzo\": \"03\", \"Abril\": \"04\", \"Mayo\": \"05\",\n","    \"Junio\": \"06\", \"Julio\": \"07\", \"Agosto\": \"08\", \"Septiembre\": \"09\",\n","    \"Octubre\": \"10\", \"Noviembre\": \"11\", \"Diciembre\": \"12\"\n","}\n","#crear la expresion para aplicar el diccionario\n","mapping_expr = create_map([lit(x) for x in sum(month_mapping.items(), ())])\n","\n","# Transformar el DataFrame\n","melted_df = melted_df.withColumn(\"mes\", mapping_expr[col(\"mes\")]) \\\n","                           .withColumn(\"dia\", lpad(col(\"dia\"), 2, \"0\"))\n","display(melted_df)\n","\n","# Crear la columna de fecha\n","df_final = melted_df.withColumn(\n","    \"fecha\",\n","    to_date(concat_ws(\"-\", col(\"dia\"), col(\"mes\"), lit(\"2024\")), \"dd-MM-yyyy\")\n",")\n","\n","# Mostrar el DataFrame transformado\n","\n","display(df_final)\n","\n","\n","\n","# Crear una columna de fecha combinando día, mes y año\n","#melted_df = melted_df.withColumn(\n","#    \"fecha\",\n","#    to_date(concat_ws(\"-\", col(\"dia\"), col(\"mes\"), lit(\"2024\")), \"dd-MM-yyyy\")\n","#)\n","\n","# Filtrar las filas inválidas (fechas nulas)\n","#melted_df = melted_df.filter(col(\"fecha\").isNotNull())\n","\n","# Formatear la columna 'valor' a float (manejar separadores de miles)\n","\n","#melted_df = melted_df.withColumn(\n","#    \"valor\",\n","#    regexp_replace(col(\"valor\"), r\"[.,]\", \"\").cast(\"float\")\n","#)\n","\n","# Mostrar el DataFrame resultante\n","#melted_df.show()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"74cc2e1c-bae4-46ec-b1be-2873c102c2be"},{"cell_type":"code","source":["#Vamos a escribir en la talba landing_eur\n","\n","df_export = df_final.select(\"fecha\",\"valor\")\n","df_export.limit(10).show()\n","\n","tablafinal = \"landing_eur\"\n","df_export.write.mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(tablafinal)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"e8a9659e-bd3e-450b-9b3a-b74a237985ad"},{"cell_type":"code","source":["df = spark.sql(\"SELECT * FROM LH_Taller.landing_eur LIMIT 1000\")\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"059d2648-7599-4f3f-9b06-c320f7a84330"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"synapse_widget":{"version":"0.1","state":{}},"dependencies":{"warehouse":{},"lakehouse":{"default_lakehouse":"b884c714-920f-4eac-9db0-1c3eb8c9f72a","default_lakehouse_name":"LH_Taller","default_lakehouse_workspace_id":"791510fa-eb31-4abc-8695-19753678d571"}}},"nbformat":4,"nbformat_minor":5}